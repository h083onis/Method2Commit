{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-110m-embedding\"\n",
    "device = \"cpu\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/camel_data4.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = df.iloc[0]['snippet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[43mcode\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'code' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(code, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(len(tokenizer.encode(row['snippet'])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m token_len \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[43m[\u001b[49m\u001b[43mtoken_len\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnippet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m  \n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m token_len \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m [token_len\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnippet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]  \n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2715\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2679\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2713\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[0;32m   2714\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2715\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2718\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2724\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3127\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3118\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3119\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3120\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3125\u001b[0m )\n\u001b[1;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3130\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta_fast.py:235\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._encode_plus\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m )\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    580\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    599\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[0;32m    600\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[1;32m--> 601\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta_fast.py:225\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m )\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:528\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[1;32m--> 528\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    540\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    542\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    552\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token_len = []\n",
    "\n",
    "\n",
    "[token_len.append(len(tokenizer.encode(row['snippet']))) for index, row in df.iterrows()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8710"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512トークン以下の割合: 0.71\n"
     ]
    }
   ],
   "source": [
    "count_512_or_less = sum(1 for length in token_len if length <= 512)\n",
    "total_count = len(token_len)\n",
    "proportion_512_or_less = count_512_or_less / total_count\n",
    "\n",
    "print(f'512トークン以下の割合: {proportion_512_or_less:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_len, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('トークンの長さの分布')\n",
    "plt.xlabel('トークンの長さ')\n",
    "plt.ylabel('頻度')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:409\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    407\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m--> 409\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding \u001b[38;5;241m=\u001b[39m model(\u001b[43mtokens\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension of the embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with norm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding\u001b[38;5;241m.\u001b[39mnorm()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Dimension of the embedding: 256, with norm=1.0\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = model(tokens)[0]\n",
    "print(f'Dimension of the embedding: {embedding.size()[0]}, with norm={embedding.norm().item()}')\n",
    "# Dimension of the embedding: 256, with norm=1.0\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>snippet</th>\n",
       "      <th>ast</th>\n",
       "      <th>is_defect</th>\n",
       "      <th>kinds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public static &lt;E&gt; Predicate&lt;E&gt; and(final Predi...</td>\n",
       "      <td>(method_declaration(modifiers(public, public)(...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public ProcessorBuilder&lt;E&gt; to(Endpoint&lt;E&gt; endp...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public Predicate&lt;E&gt; getPredicate() {\\n        ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public Predicate&lt;E&gt; headerEquals(final String ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a0bad2c4a35be88f7732def261a03737fab662e</td>\n",
       "      <td>public DestinationBuilder&lt;E&gt; otherwise() {\\n  ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  commit_id  \\\n",
       "0  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "1  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "2  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "3  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "4  0a0bad2c4a35be88f7732def261a03737fab662e   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  public static <E> Predicate<E> and(final Predi...   \n",
       "1  public ProcessorBuilder<E> to(Endpoint<E> endp...   \n",
       "2  public Predicate<E> getPredicate() {\\n        ...   \n",
       "3  public Predicate<E> headerEquals(final String ...   \n",
       "4  public DestinationBuilder<E> otherwise() {\\n  ...   \n",
       "\n",
       "                                                 ast  is_defect  kinds  \n",
       "0  (method_declaration(modifiers(public, public)(...       True  train  \n",
       "1  (method_declaration(modifiers(public, public))...       True  train  \n",
       "2  (method_declaration(modifiers(public, public))...       True  train  \n",
       "3  (method_declaration(modifiers(public, public))...       True  train  \n",
       "4  (method_declaration(modifiers(public, public))...       True  train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 516096 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Filter the DataFrame to keep only records where token length is 512 or less\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m df_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet_vec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnippet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the filtered DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_filtered)\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Filter the DataFrame to keep only records where token length is 512 or less\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m df_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet_vec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the filtered DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_filtered)\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\Salesforce\\codet5p-110m-embedding\\94f88f95672b1d4b0cc715c6011001a74f892bdd\\modeling_codet5p_embedding.py:41\u001b[0m, in \u001b[0;36mCodeT5pEmbeddingModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     31\u001b[0m         input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor], BaseModelOutput]:\n\u001b[0;32m     39\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m---> 41\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1106\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1091\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1092\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1093\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1103\u001b[0m         output_attentions,\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1106\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:686\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    684\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    696\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masak\\anaconda3\\envs\\M2C\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    592\u001b[0m normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    593\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[0;32m    594\u001b[0m     normed_hidden_states,\n\u001b[0;32m    595\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    601\u001b[0m )\n\u001b[1;32m--> 602\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 516096 bytes."
     ]
    }
   ],
   "source": [
    "# Calculate token lengths and store them in a new column\n",
    "df['token_len'] = df['snippet'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "\n",
    "# Filter the DataFrame to keep only records where token length is 512 or less\n",
    "df_filtered = df[df['token_len'] <= 512]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>snippet</th>\n",
       "      <th>ast</th>\n",
       "      <th>is_defect</th>\n",
       "      <th>kinds</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public static &lt;E&gt; Predicate&lt;E&gt; and(final Predi...</td>\n",
       "      <td>(method_declaration(modifiers(public, public)(...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public ProcessorBuilder&lt;E&gt; to(Endpoint&lt;E&gt; endp...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public Predicate&lt;E&gt; getPredicate() {\\n        ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4701ac4bf5b50810914796e284b0e3f78a240bc</td>\n",
       "      <td>public Predicate&lt;E&gt; headerEquals(final String ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a0bad2c4a35be88f7732def261a03737fab662e</td>\n",
       "      <td>public DestinationBuilder&lt;E&gt; otherwise() {\\n  ...</td>\n",
       "      <td>(method_declaration(modifiers(public, public))...</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  commit_id  \\\n",
       "0  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "1  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "2  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "3  a4701ac4bf5b50810914796e284b0e3f78a240bc   \n",
       "4  0a0bad2c4a35be88f7732def261a03737fab662e   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  public static <E> Predicate<E> and(final Predi...   \n",
       "1  public ProcessorBuilder<E> to(Endpoint<E> endp...   \n",
       "2  public Predicate<E> getPredicate() {\\n        ...   \n",
       "3  public Predicate<E> headerEquals(final String ...   \n",
       "4  public DestinationBuilder<E> otherwise() {\\n  ...   \n",
       "\n",
       "                                                 ast  is_defect  kinds  \\\n",
       "0  (method_declaration(modifiers(public, public)(...       True  train   \n",
       "1  (method_declaration(modifiers(public, public))...       True  train   \n",
       "2  (method_declaration(modifiers(public, public))...       True  train   \n",
       "3  (method_declaration(modifiers(public, public))...       True  train   \n",
       "4  (method_declaration(modifiers(public, public))...       True  train   \n",
       "\n",
       "   token_len  \n",
       "0        192  \n",
       "1        117  \n",
       "2         38  \n",
       "3        161  \n",
       "4         65  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_filtered.drop(columns=['token_len'])\n",
    "\n",
    "df_filtered.to_csv('camel_filtered_data4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
